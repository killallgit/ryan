# Ryan - Project Overview

## What is Ryan?

Ryan is a responsive terminal-based chat interface for AI assistants, starting with Ollama integration. Built with Go and tcell for a smooth, non-blocking user experience.

## Features

ğŸš€ **Non-Blocking Interface** - UI stays responsive during API calls  
âš¡ **Real-time Feedback** - Immediate spinner visibility with progress tracking  
ğŸ¯ **Enhanced Error Display** - Clear error messages with base16 red colors  
â° **Progress Tracking** - Elapsed time display for long-running operations  
ğŸ”Œ **Connectivity Validation** - Automatic Ollama server status checking  
âŒ¨ï¸ **Escape Key Cancellation** - Cancel operations with Escape key  
ğŸ“± **Responsive Layout** - Adapts gracefully to terminal resizing  
ğŸ¨ **Clean Architecture** - Functional programming with immutable data structures  
ğŸ› ï¸ **Universal Tool System** - Industry-standard tool calling compatible with all major LLM providers  
ğŸ”§ **Built-in Tools** - Bash command execution and file reading with safety constraints  
ğŸ”’ **Security First** - Comprehensive safety validation and sandboxing for tool execution  

## UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Message Area            â”‚ â† Chat history with scroll
â”‚    User: Hello there            â”‚
â”‚    Assistant: Hi! How can I..   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Alert Area              â”‚ â† Spinner, errors, progress  
â”‚    ğŸ”„ Sending... (5s)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Input Area              â”‚ â† Type your message here
â”‚    > |                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Status Area              â”‚ â† Model info, connection status
â”‚    Model: llama3.1:8b | Ready  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
ryan/
â”œâ”€â”€ cmd/ryan/           # Main CLI entry point
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ chat/          # Core chat domain logic
â”‚   â”œâ”€â”€ controllers/   # Business logic orchestration  
â”‚   â”œâ”€â”€ ollama/        # Ollama API client
â”‚   â”œâ”€â”€ tools/         # Universal tool system
â”‚   â””â”€â”€ tui/           # Terminal user interface
â”œâ”€â”€ docs/              # Architecture and design docs
â”œâ”€â”€ examples/          # Tool system demos and examples
â”œâ”€â”€ integration/       # Integration tests
â””â”€â”€ Taskfile.yaml      # Development tasks
```

## Configuration Options

### Environment Variables

- `RYAN_OLLAMA_URL` - Ollama server URL (default: http://localhost:11434)
- `RYAN_MODEL` - Default model to use (default: llama3.1:8b)
- `RYAN_CONFIG_DIR` - Configuration directory (default: ~/.ryan)

### Config File Format

```yaml
# ~/.ryan/settings.yaml
ollama:
  url: "http://localhost:11434"
  model: "llama3.1:8b"
  timeout: "60s"
  system_prompt: "You are a helpful assistant."

ui:
  theme: "default"
  spinner_interval: "100ms"
  max_history: 1000

tools:
  enabled: true
  bash:
    enabled: true
    timeout: "30s"
    allowed_paths: ["~/", "/tmp"]
  file_read:
    enabled: true
    max_file_size: "10MB"
    allowed_extensions: [".txt", ".md", ".go", ".json"]

logging:
  level: "info"
  file: "~/.ryan/logs/app.log"
```

## Development Phases

Ryan was built using an incremental complexity approach:

### âœ… Phase 1: Foundation (COMPLETED)
- Core message and conversation types
- Synchronous HTTP client for Ollama API  
- Basic chat controller
- Simple CLI interface for testing

### âœ… Phase 2: Non-Blocking TUI (COMPLETED)
- Event-driven TUI with tcell
- Non-blocking message sending
- Enhanced spinner visibility and progress tracking
- Alert area with base16 red error colors
- Ollama connectivity validation
- Escape key cancellation support

### ğŸš§ Phase 3: Streaming Infrastructure (CURRENT)
- HTTP streaming client
- Message chunk accumulation
- Streaming controller with channels

### ğŸ“‹ Phase 4: TUI + Streaming Integration (PLANNED)
- Real-time message streaming in TUI
- Progressive message display
- Thread-safe streaming updates

### ğŸ¨ Phase 5: Polish & Production (PLANNED)
- Advanced UI features (syntax highlighting, themes)
- Performance optimization
- Configuration and customization options

## Testing Strategy

The project follows test-driven development with comprehensive coverage:

- **Unit Tests** - Pure functions and isolated components
- **Integration Tests** - Real API communication with Ollama
- **TUI Tests** - Event simulation and screen capture
- **Concurrency Tests** - Race detection and deadlock prevention

```shell
# Run with race detection
go test -race ./...

# Run with coverage
go test -cover ./...
```

## Troubleshooting

### Common Issues

**Connection refused errors**
- Check that Ollama is running: `ollama serve`
- Verify URL in config: `~/.ryan/settings.yaml`
- Test connectivity: `curl http://localhost:11434/api/tags`

**UI freezing during API calls**
- This was a known issue fixed in Phase 2
- The current version uses non-blocking architecture

**Long response times**
- Large models may take time to respond
- Progress is shown with elapsed time tracking
- Use Escape key to cancel if needed