# Ryan - Project Overview

## What is Ryan?

Ryan is a responsive terminal-based chat interface for AI assistants, starting with Ollama integration. Built with Go and tcell for a smooth, non-blocking user experience.

## Features

ğŸš€ **Non-Blocking Interface** - UI stays responsive during API calls  
âš¡ **Real-time Feedback** - Immediate spinner visibility with progress tracking  
ğŸ¯ **Enhanced Error Display** - Clear error messages with base16 red colors  
â° **Progress Tracking** - Elapsed time display for long-running operations  
ğŸ”Œ **Connectivity Validation** - Automatic Ollama server status checking  
âŒ¨ï¸ **Escape Key Cancellation** - Cancel operations with Escape key  
ğŸ“± **Responsive Layout** - Adapts gracefully to terminal resizing  
ğŸ¨ **Clean Architecture** - Functional programming with immutable data structures  
ğŸ› ï¸ **Advanced Tool System** - Claude Code-level tool execution with concurrent orchestration  
ğŸ”§ **Comprehensive Tool Suite** - 15+ production-ready tools with batch execution capability  
ğŸ”’ **Security First** - Comprehensive safety validation and sandboxing for tool execution  
ğŸ”„ **Multi-Provider Support** - Universal tool calling for OpenAI, Anthropic, and Ollama  

## UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Message Area            â”‚ â† Chat history with scroll
â”‚    User: Hello there            â”‚
â”‚    Assistant: Hi! How can I..   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Alert Area              â”‚ â† Spinner, errors, progress  
â”‚    ğŸ”„ Sending... (5s)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Input Area              â”‚ â† Type your message here
â”‚    > |                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Status Area              â”‚ â† Model info, connection status
â”‚    Model: llama3.1:8b | Ready  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
ryan/
â”œâ”€â”€ cmd/ryan/           # Main CLI entry point
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ chat/          # Core chat domain logic
â”‚   â”œâ”€â”€ controllers/   # Business logic orchestration  
â”‚   â”œâ”€â”€ ollama/        # Ollama API client
â”‚   â”œâ”€â”€ providers/     # Multi-provider adapters (OpenAI, Anthropic)
â”‚   â”œâ”€â”€ tools/         # Advanced tool execution system
â”‚   â””â”€â”€ tui/           # Terminal user interface
â”œâ”€â”€ docs/              # Architecture and design docs
â”œâ”€â”€ examples/          # Tool system demos and examples
â”œâ”€â”€ integration/       # Integration tests
â””â”€â”€ Taskfile.yaml      # Development tasks
```

## Configuration Options

### Environment Variables

- `RYAN_OLLAMA_URL` - Ollama server URL (default: http://localhost:11434)
- `RYAN_MODEL` - Default model to use (default: llama3.1:8b)
- `RYAN_CONFIG_DIR` - Configuration directory (default: ~/.ryan)

### Config File Format

```yaml
# ~/.ryan/settings.yaml
ollama:
  url: "http://localhost:11434"
  model: "llama3.1:8b"
  timeout: "60s"
  system_prompt: "You are a helpful assistant."

ui:
  theme: "default"
  spinner_interval: "100ms"
  max_history: 1000

tools:
  enabled: true
  bash:
    enabled: true
    timeout: "30s"
    allowed_paths: ["~/", "/tmp"]
  file_read:
    enabled: true
    max_file_size: "10MB"
    allowed_extensions: [".txt", ".md", ".go", ".json"]

logging:
  level: "info"
  file: "~/.ryan/logs/app.log"
```

## Development Status

Ryan follows an incremental development approach with clear phases:

### âœ… Phase 1: Foundation (COMPLETED)
- Core message and conversation types
- Synchronous HTTP client for Ollama API  
- Basic chat controller with conversation management
- Configuration system with Viper integration

### âœ… Phase 2: Non-Blocking TUI (COMPLETED)
- Event-driven TUI with tcell framework
- Non-blocking message sending with goroutines
- Enhanced spinner and progress tracking
- Alert area with error display and base16 colors
- Ollama connectivity validation and user guidance
- Escape key cancellation for long operations

### âœ… Phase 3A: Streaming Implementation (COMPLETED)
- âœ… HTTP streaming client with chunk processing
- âœ… Message accumulation with Unicode handling
- âœ… Thread-safe streaming updates in TUI
- âœ… Real-time message display during streaming
- âœ… Automatic fallback for non-streaming clients
- âœ… Error recovery and connection management

### âœ… Phase 3B: Production Tool System (COMPLETED)
- âœ… Universal tool interface supporting multiple providers
- âœ… Built-in tools: `execute_bash` and `read_file` with safety constraints
- âœ… Provider adapters for OpenAI, Anthropic, Ollama formats
- âœ… Complete tool calling integration with streaming chat
- âœ… Comprehensive safety framework and security validation
- âœ… Model compatibility validation (41 compatible models identified)

### ğŸš§ Phase 3C: Tool System Expansion (IN PROGRESS)
- ğŸš§ Advanced tool execution engine with concurrent orchestration
- ğŸš§ Expanded tool suite (WebFetch, Grep, Git integration, etc.)
- ğŸš§ Batch tool execution ("multiple tools in single response")
- ğŸš§ Real-time tool progress tracking in TUI

### ğŸ“‹ Phase 4: Advanced Features (PLANNED)
- User consent system for dangerous operations
- Tool execution sandboxing and resource limits
- Audit logging and execution history
- MCP protocol support for external tool providers

### ğŸ¨ Phase 5: Polish & Optimization (PLANNED)
- Advanced UI features (syntax highlighting, themes)
- Performance optimization and result caching
- Custom tool development SDK
- Enhanced configuration and customization options

## Testing Strategy

The project follows test-driven development with comprehensive coverage:

- **Unit Tests** - Pure functions and isolated components
- **Integration Tests** - Real API communication with Ollama
- **TUI Tests** - Event simulation and screen capture
- **Concurrency Tests** - Race detection and deadlock prevention

```shell
# Run with race detection
go test -race ./...

# Run with coverage
go test -cover ./...
```

## Troubleshooting

### Common Issues

**Connection refused errors**
- Check that Ollama is running: `ollama serve`
- Verify URL in config: `~/.ryan/settings.yaml`
- Test connectivity: `curl http://localhost:11434/api/tags`

**UI freezing during API calls**
- This was a known issue fixed in Phase 2
- The current version uses non-blocking architecture

**Long response times**
- Large models may take time to respond
- Progress is shown with elapsed time tracking
- Use Escape key to cancel if needed