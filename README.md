# Ryan - Interactive Chat TUI

## Claude's friend.

A responsive terminal-based chat interface for AI assistants, starting with Ollama integration. Built with Go and tcell for a smooth, non-blocking user experience.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Go Version](https://img.shields.io/badge/go-1.19+-blue.svg)
![Build Status](https://img.shields.io/badge/build-passing-green.svg)

## Features

ğŸš€ **Non-Blocking Interface** - UI stays responsive during API calls  
âš¡ **Real-time Feedback** - Immediate spinner visibility with progress tracking  
ğŸ¯ **Enhanced Error Display** - Clear error messages with base16 red colors  
â° **Progress Tracking** - Elapsed time display for long-running operations  
ğŸ”Œ **Connectivity Validation** - Automatic Ollama server status checking  
âŒ¨ï¸ **Escape Key Cancellation** - Cancel operations with Escape key  
ğŸ“± **Responsive Layout** - Adapts gracefully to terminal resizing  
ğŸ¨ **Clean Architecture** - Functional programming with immutable data structures  

## Quick Start

```shell
# Start a chat session
ryan

# Available keyboard shortcuts:
# Enter - Send message
# Escape - Cancel operation or quit
# Arrow keys - Navigate message history
# Page Up/Down - Scroll through messages
```

## Installation

### Prerequisites

- Go 1.19 or later
- Ollama server running locally (default: http://localhost:11434)

### From Source

```shell
# Clone the repository
git clone https://github.com/your-org/ryan.git
cd ryan

# Install dependencies
go mod download

# Build and install
go build -o ryan ./cmd/ryan
```

### Configuration

Create a configuration file at `~/.ryan/config.yaml`:

```yaml
ollama:
  url: "http://localhost:11434"
  model: "llama3.1:8b"

ui:
  theme: "default"
  spinner_interval: "100ms"
```

## Architecture

Ryan follows a clean, functional architecture with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TUI Layer     â”‚    â”‚  Controller     â”‚    â”‚   Chat Core     â”‚
â”‚   (tcell)       â”‚â”€â”€â”€â”€â”‚    Layer        â”‚â”€â”€â”€â”€â”‚   (business)    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                       â”‚                       â”‚
        â–¼                       â–¼                       â–¼
   UI Events              Orchestration            API Calls
   Rendering              State Management         Message Logic
```

### Key Components

- **Chat Domain** (`pkg/chat/`) - Core business logic, API-agnostic
- **Ollama Integration** (`pkg/ollama/`) - HTTP client for Ollama API
- **TUI Components** (`pkg/tui/`) - Terminal interface using tcell
- **Controllers** (`pkg/controllers/`) - Orchestration between layers

### UI Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Message Area            â”‚ â† Chat history with scroll
â”‚    User: Hello there            â”‚
â”‚    Assistant: Hi! How can I..   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Alert Area              â”‚ â† Spinner, errors, progress  
â”‚    ğŸ”„ Sending... (5s)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Input Area              â”‚ â† Type your message here
â”‚    > |                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        Status Area              â”‚ â† Model info, connection status
â”‚    Model: llama3.1:8b | Ready  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Development

### Project Structure

```
ryan/
â”œâ”€â”€ cmd/ryan/           # Main CLI entry point
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ chat/          # Core chat domain logic
â”‚   â”œâ”€â”€ controllers/   # Business logic orchestration  
â”‚   â”œâ”€â”€ ollama/        # Ollama API client
â”‚   â””â”€â”€ tui/           # Terminal user interface
â”œâ”€â”€ docs/              # Architecture and design docs
â”œâ”€â”€ tests/             # Integration tests
â””â”€â”€ Taskfile.yml       # Development tasks
```

### Development Workflow

```shell
# Run tests
task test

# Run integration tests (requires Ollama)
task test:integration  

# Run all tests
task test:all

# Build for development
task build

# Run linter
task lint
```

### Testing

The project follows test-driven development with comprehensive coverage:

- **Unit Tests** - Pure functions and isolated components
- **Integration Tests** - Real API communication with Ollama
- **TUI Tests** - Event simulation and screen capture
- **Concurrency Tests** - Race detection and deadlock prevention

```shell
# Run with race detection
go test -race ./...

# Run with coverage
go test -cover ./...
```

## Implementation Phases

Ryan is built using an incremental complexity approach:

### âœ… Phase 1: Foundation (COMPLETED)
- Core message and conversation types
- Synchronous HTTP client for Ollama API  
- Basic chat controller
- Simple CLI interface for testing

### âœ… Phase 2: Non-Blocking TUI (COMPLETED)
- Event-driven TUI with tcell
- Non-blocking message sending
- Enhanced spinner visibility and progress tracking
- Alert area with base16 red error colors
- Ollama connectivity validation
- Escape key cancellation support

### ğŸš§ Phase 3: Streaming Infrastructure (NEXT)
- HTTP streaming client
- Message chunk accumulation
- Streaming controller with channels

### ğŸ“‹ Phase 4: TUI + Streaming Integration (PLANNED)
- Real-time message streaming in TUI
- Progressive message display
- Thread-safe streaming updates

### ğŸ¨ Phase 5: Polish & Production (PLANNED)
- Advanced UI features (syntax highlighting, themes)
- Performance optimization
- Configuration and customization options

## Contributing

We welcome contributions! Please see our development workflow:

1. **Functional Programming First** - Prefer pure functions over stateful objects
2. **Test-Driven Development** - Write tests before implementation
3. **Phase-Based Development** - Never skip phases or add complexity ahead of schedule
4. **Clean Architecture** - Maintain clear separation of concerns

### Code Style

- Use functional paradigms wherever possible
- Avoid OOP principles and "utilities" 
- Write self-documenting, descriptive names
- Follow Go conventions and idioms
- No code comments unless explicitly needed

## Configuration

### Environment Variables

- `RYAN_OLLAMA_URL` - Ollama server URL (default: http://localhost:11434)
- `RYAN_MODEL` - Default model to use (default: llama3.1:8b)
- `RYAN_CONFIG_DIR` - Configuration directory (default: ~/.ryan)

### Config File Options

```yaml
# ~/.ryan/config.yaml
ollama:
  url: "http://localhost:11434"
  model: "llama3.1:8b"
  timeout: "60s"

ui:
  theme: "default"
  spinner_interval: "100ms"
  max_history: 1000

logging:
  level: "info"
  file: "~/.ryan/logs/app.log"
```

## Troubleshooting

### Common Issues

**Spinner not visible when sending messages**
- Fixed in recent version with immediate state synchronization
- Ensure you're running the latest version

**Connection refused errors**
- Check that Ollama is running: `ollama serve`
- Verify URL in config: `~/.ryan/config.yaml`
- Test connectivity: `curl http://localhost:11434/api/tags`

**UI freezing during API calls**
- This was a known issue fixed in Phase 2
- The current version uses non-blocking architecture

**Long response times**
- Large models may take time to respond
- Progress is shown with elapsed time tracking
- Use Escape key to cancel if needed

### Getting Help

- Check the [documentation](./docs/) for detailed architecture info
- Review [TUI patterns](./docs/TUI_PATTERNS.md) for interface behavior
- See [development roadmap](./docs/DEVELOPMENT_ROADMAP.md) for current status

## License

MIT License - see [LICENSE](LICENSE) for details.

## Acknowledgments

- [tcell](https://github.com/gdamore/tcell) - Terminal control library
- [Ollama](https://ollama.ai/) - Local AI model serving
- [Cobra](https://github.com/spf13/cobra) - CLI framework
- [Viper](https://github.com/spf13/viper) - Configuration management

---

**Status**: Phase 2 Complete âœ… | **Next**: Streaming Implementation ğŸš§  
**Architecture**: Production Ready ğŸ—ï¸ | **UX**: Significantly Enhanced ğŸš€