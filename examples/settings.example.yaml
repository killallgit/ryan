logging:
    file: ./.ryan/debug.log
    preserve: false

show_thinking: true
streaming: true

ollama:
    model: qwen3:latest
    system_prompt: examples/SYSTEM_PROMPT.md
    url: https://ollama.kitty-tetra.ts.net
    poll_interval: 10
    timeout: "1h"
    # Enable LangChain Go integration (optional, defaults to false)
    # This provides standardized LLM interface, better error handling,
    # and foundation for multi-provider support
    use_langchain: false

tools:
    enabled: true
    truncate_output: true
    models:
    - deepseek-r1:latest
    - qwen3:latest
    - llama3.1:latest
    - qwen2.5-coder:7b
    - mistral-small:latest
    bash:
        enabled: true
        timeout: "90s"
        allowed_paths: [".", "/tmp"]
    file_read:
        enabled: true
        max_file_size: "10MB"
        allowed_extensions: [".txt", ".md", ".go", ".json"]
    search:
        enabled: true
        timeout: "10s"
        