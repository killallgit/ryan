logging:
    file: ./.ryan/debug.log
    preserve: false

show_thinking: true
streaming: true

ollama:
    model: qwen3:latest
    system_prompt: examples/SYSTEM_PROMPT.md
    url: https://ollama.kitty-tetra.ts.net
    timeout: "1h"

tools:
    models:                         # Models that support tool calling
    - deepseek-r1:latest
    - qwen3:latest
    - llama3.1:latest
    - qwen2.5-coder:7b
    - mistral-small:latest
    
    bash:
        enabled: true               # Enable bash command execution
        timeout: "90s"              # Maximum execution time per command
        allowed_paths: [".", "/tmp"] # Directories where commands can run
        skip_permissions: false     # Skip all safety checks (dangerous!)
        
    file_read:
        enabled: true               # Enable file reading
        max_file_size: "10MB"       # Maximum file size to read
        allowed_extensions:         # Allowed file extensions
            - ".txt"
            - ".md"
            - ".go"
            - ".json"
            - ".yaml"
            - ".yml"
            - ".py"
            - ".js"
            - ".ts"

# LangChain configuration for enhanced agent capabilities            
langchain:
    tools:
        max_iterations: 5           # Maximum agent iterations
    memory:
        type: "buffer"              # Memory type: "buffer" or "window"
        window_size: 10             # For window memory type
        max_tokens: 4000            # Maximum tokens in memory
        summary_threshold: 1000     # Token threshold for summarization
    prompts:
        context_injection: true     # Enable context injection
        