# Ryan Configuration Reference
# Complete configuration file with all available options
# Copy this file to .ryan/settings.yaml and customize as needed

# Logging
logging:
  log_file: ./.ryan/system.log
  preserve: false
  level: info

# Context persistence
context:
  directory: ./.ryan/contexts
  max_file_size: 10MB
  persist_langchain: true

# UI settings
show_thinking: true
streaming: true

# Provider selection (ollama, openai)
provider: ollama  # Default provider to use

# Ollama LLM configuration
ollama:
  url: http://localhost:11434
  model: qwen2.5-coder:7b  # Recommended for code analysis
  system_prompt: "examples/SYSTEM_PROMPT.md"
  timeout: 90s
  poll_interval: 10

# OpenAI configuration (for future implementation)
openai:
  api_key: ""  # Set via environment variable OPENAI_API_KEY
  model: gpt-4-turbo-preview
  system_prompt: "examples/SYSTEM_PROMPT.md"
  timeout: 60s
  base_url: ""  # Optional: for Azure or custom endpoints

# Tools
tools:
  enabled: true
  truncate_output: true
  bash:
    enabled: true
    timeout: 90s
    allowed_paths: []
    skip_permissions: false
  file_read:
    enabled: true
    max_file_size: 10MB
    allowed_extensions: []
  search:
    enabled: true
    timeout: 10s

# LangChain
langchain:
  tools:
    max_iterations: 5
    autonomous_reasoning: true
    use_react_pattern: true
    verbose_logging: false
  memory:
    type: buffer
    window_size: 10
    max_tokens: 4000
    summary_threshold: 1000
  prompts:
    context_injection: true

# Vector Store
vectorstore:
  enabled: true
  provider: chromem
  persistence_dir: ./.ryan/vectorstore
  enable_persistence: true
  embedder:
    provider: ollama
    model: nomic-embed-text
    base_url: http://localhost:11434
    api_key: ""
  collections:
    - name: conversations
      metadata:
        type: chat_history
    - name: documents
      metadata:
        type: document_index
  indexer:
    chunk_size: 1000
    chunk_overlap: 200
    auto_index: false
