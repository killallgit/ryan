# Example Ryan configuration with vector store settings

# Basic Ryan settings
show_thinking: true
streaming: true

# Logging configuration
logging:
  file: ./.ryan/logs/debug.log
  preserve: false
  level: info

# Context persistence
context:
  directory: ./.ryan/contexts
  history_file: ./.ryan/logs/debug.history
  max_file_size: 10MB
  persist_langchain: true

# Ollama configuration
ollama:
  url: http://localhost:11434
  model: qwen3:latest
  timeout: 90s
  poll_interval: 10

# Vector store configuration
vectorstore:
  enabled: true
  provider: chromem  # Currently only chromem is supported
  persistence_dir: ./.ryan/vectorstore
  enable_persistence: true
  
  # Embedder configuration
  embedder:
    # For local embeddings with Ollama
    provider: ollama
    model: nomic-embed-text
    base_url: http://localhost:11434
    
    # For OpenAI embeddings (uncomment to use)
    # provider: openai
    # model: text-embedding-3-small
    # api_key: ""  # Set via OPENAI_API_KEY environment variable
    
    # For testing (mock embeddings)
    # provider: mock
  
  # Pre-create collections on startup
  collections:
    - name: conversations
      metadata:
        type: chat_history
        description: Stores conversation history for semantic retrieval
    
    - name: documents
      metadata:
        type: document_index
        description: Indexed documents for RAG
    
    - name: code_snippets
      metadata:
        type: code
        description: Code examples and snippets
  
  # Document indexer configuration
  indexer:
    chunk_size: 1000      # Characters per chunk
    chunk_overlap: 200    # Overlap between chunks
    auto_index: false     # Auto-index new files

# LangChain configuration
langchain:
  tools:
    max_iterations: 5
    autonomous_reasoning: true
    use_react_pattern: true
    verbose_logging: false
  
  memory:
    type: buffer          # Can be: buffer, vector, summary
    window_size: 10
    max_tokens: 4000
    summary_threshold: 1000
    # When using vector memory
    # type: vector
    # collection: conversations
    # max_retrieved: 5
    # score_threshold: 0.7
  
  prompts:
    context_injection: true

# Tools configuration
tools:
  enabled: true
  truncate_output: true
  
  bash:
    enabled: true
    timeout: 90s
    allowed_paths: []
    skip_permissions: false
  
  file_read:
    enabled: true
    max_file_size: 10MB
    allowed_extensions: []
  
  search:
    enabled: true
    timeout: 10s
    use_langchain: false